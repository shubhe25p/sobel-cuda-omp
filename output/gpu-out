nblocks: 1, nThreads: 32
==PROF== Connected to process 1937727 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 32 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1937727
[1937727] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:15:49, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.01
    gpu__time_duration.avg                                                          second                           3.25
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.23
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         765.00
    Elapsed Cycles                                                                   cycle                  2,488,007,743
    Memory [%]                                                                           %                           0.05
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                        second                           3.25
    L1/TEX Cache Throughput                                                              %                           5.73
    L2 Cache Throughput                                                                  %                           0.05
    SM Active Cycles                                                                 cycle                  23,037,274.43
    Compute (SM) [%]                                                                     %                           0.08
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             40
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory This kernel's  
          theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM The difference        
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,661
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1, nThreads: 64
==PROF== Connected to process 1948066 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 64 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1948066
[1948066] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:18:20, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.01
    gpu__time_duration.avg                                                          second                           1.65
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.46
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         765.00
    Elapsed Cycles                                                                   cycle                  1,263,941,366
    Memory [%]                                                                           %                           0.10
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                        second                           1.65
    L1/TEX Cache Throughput                                                              %                          11.27
    L2 Cache Throughput                                                                  %                           0.09
    SM Active Cycles                                                                 cycle                  11,704,084.94
    Compute (SM) [%]                                                                     %                           0.16
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             64
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             20
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           3.11
    Achieved Active Warps Per SM                                                      warp                           1.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (3.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,662
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1, nThreads: 128
==PROF== Connected to process 1954022 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 128 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1954022
[1954022] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:19:38, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.02
    gpu__time_duration.avg                                                         msecond                         855.45
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.91
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.05
    Elapsed Cycles                                                                   cycle                    654,455,472
    Memory [%]                                                                           %                           0.20
    DRAM Throughput                                                                      %                           0.02
    Duration                                                                       msecond                         855.45
    L1/TEX Cache Throughput                                                              %                          21.75
    L2 Cache Throughput                                                                  %                           0.17
    SM Active Cycles                                                                 cycle                   6,059,421.59
    Compute (SM) [%]                                                                     %                           0.31
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            128
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             10
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           6.15
    Achieved Active Warps Per SM                                                      warp                           3.93
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (6.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,664
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1, nThreads: 256
==PROF== Connected to process 1956580 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1956580
[1956580] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:20:23, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.04
    gpu__time_duration.avg                                                         msecond                         524.74
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.90
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.99
    Elapsed Cycles                                                                   cycle                    401,424,344
    Memory [%]                                                                           %                           0.33
    DRAM Throughput                                                                      %                           0.04
    Duration                                                                       msecond                         524.74
    L1/TEX Cache Throughput                                                              %                          35.77
    L2 Cache Throughput                                                                  %                           0.27
    SM Active Cycles                                                                 cycle                   3,715,439.52
    Compute (SM) [%]                                                                     %                           0.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              5
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                          12.04
    Achieved Active Warps Per SM                                                      warp                           7.71
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (12.0%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,668
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1, nThreads: 512
==PROF== Connected to process 1958009 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 512 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1958009
[1958009] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:20:54, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.05
    gpu__time_duration.avg                                                         msecond                         396.62
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.89
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.64
    Elapsed Cycles                                                                   cycle                    303,668,349
    Memory [%]                                                                           %                           0.49
    DRAM Throughput                                                                      %                           0.05
    Duration                                                                       msecond                         396.62
    L1/TEX Cache Throughput                                                              %                          52.54
    L2 Cache Throughput                                                                  %                           0.37
    SM Active Cycles                                                                 cycle                   2,810,396.24
    Compute (SM) [%]                                                                     %                           0.67
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            512
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          23.55
    Achieved Active Warps Per SM                                                      warp                          15.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers The difference     
          between calculated theoretical (50.0%) and measured achieved occupancy (23.6%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,676
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1, nThreads: 1024
==PROF== Connected to process 1959478 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1 blocks, 1024 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1959478
[1959478] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:21:22, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.05
    gpu__time_duration.avg                                                         msecond                         391.91
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.89
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.61
    Elapsed Cycles                                                                   cycle                    299,658,511
    Memory [%]                                                                           %                           0.54
    DRAM Throughput                                                                      %                           0.05
    Duration                                                                       msecond                         391.91
    L1/TEX Cache Throughput                                                              %                          58.71
    L2 Cache Throughput                                                                  %                           0.48
    SM Active Cycles                                                                 cycle                   2,772,558.77
    Compute (SM) [%]                                                                     %                           0.68
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.01
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              1
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          43.91
    Achieved Active Warps Per SM                                                      warp                          28.10
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,692
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4, nThreads: 32
==PROF== Connected to process 1961305 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4 blocks, 32 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1961305
[1961305] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:22:39, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.02
    gpu__time_duration.avg                                                         msecond                         828.13
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           0.91
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.99
    Elapsed Cycles                                                                   cycle                    633,509,074
    Memory [%]                                                                           %                           0.21
    DRAM Throughput                                                                      %                           0.02
    Duration                                                                       msecond                         828.13
    L1/TEX Cache Throughput                                                              %                           5.73
    L2 Cache Throughput                                                                  %                           0.17
    SM Active Cycles                                                                 cycle                  23,062,905.34
    Compute (SM) [%]                                                                     %                           0.32
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            128
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             40
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory This kernel's  
          theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM The difference        
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,664
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4, nThreads: 64
==PROF== Connected to process 1963170 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4 blocks, 64 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1963170
[1963170] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:23:53, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.05
    gpu__time_duration.avg                                                         msecond                         428.00
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           1.78
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.99
    Elapsed Cycles                                                                   cycle                    327,417,611
    Memory [%]                                                                           %                           0.40
    DRAM Throughput                                                                      %                           0.05
    Duration                                                                       msecond                         428.00
    L1/TEX Cache Throughput                                                              %                          11.26
    L2 Cache Throughput                                                                  %                           0.32
    SM Active Cycles                                                                 cycle                  11,715,104.69
    Compute (SM) [%]                                                                     %                           0.62
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            256
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             20
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           3.11
    Achieved Active Warps Per SM                                                      warp                           1.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (3.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,668
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4, nThreads: 128
==PROF== Connected to process 1966513 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4 blocks, 128 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1966513
[1966513] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:24:19, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.08
    gpu__time_duration.avg                                                         msecond                         228.60
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           3.41
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.56
    Elapsed Cycles                                                                   cycle                    174,779,265
    Memory [%]                                                                           %                           0.75
    DRAM Throughput                                                                      %                           0.08
    Duration                                                                       msecond                         228.60
    L1/TEX Cache Throughput                                                              %                          21.72
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                   6,067,860.09
    Compute (SM) [%]                                                                     %                           1.16
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            512
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             10
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           6.14
    Achieved Active Warps Per SM                                                      warp                           3.93
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (6.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,676
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4, nThreads: 256
==PROF== Connected to process 1967194 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==ERROR== Failed to prepare kernel for profiling

==ERROR== Failed to profile kernel "_Z16sobel_kernel_gpuPfS_iiiS_S_" in process 1967194
==PROF== Trying to shutdown target application
==ERROR== The application returned an error code (9).
==ERROR== An error occurred while trying to profile.
==WARNING== No kernels were profiled.
==WARNING== Profiling kernels launched by child processes requires the --target-processes all option.
nblocks: 4, nThreads: 512
==PROF== Connected to process 1967490 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4 blocks, 512 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1967490
[1967490] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:24:36, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.17
    gpu__time_duration.avg                                                         msecond                         114.52
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           3.09
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.19
    Elapsed Cycles                                                                   cycle                     87,627,951
    Memory [%]                                                                           %                           1.69
    DRAM Throughput                                                                      %                           0.17
    Duration                                                                       msecond                         114.52
    L1/TEX Cache Throughput                                                              %                          52.55
    L2 Cache Throughput                                                                  %                           1.25
    SM Active Cycles                                                                 cycle                   2,814,697.99
    Compute (SM) [%]                                                                     %                           2.32
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          2,048
    Waves Per SM                                                                                                     0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          23.53
    Achieved Active Warps Per SM                                                      warp                          15.06
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers The difference     
          between calculated theoretical (50.0%) and measured achieved occupancy (23.5%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,724
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4, nThreads: 1024
==PROF== Connected to process 1967686 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4 blocks, 1024 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1967686
[1967686] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:24:50, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.13
    gpu__time_duration.avg                                                         msecond                         148.96
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           2.34
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.97
    Elapsed Cycles                                                                   cycle                    114,098,954
    Memory [%]                                                                           %                           1.43
    DRAM Throughput                                                                      %                           0.13
    Duration                                                                       msecond                         148.96
    L1/TEX Cache Throughput                                                              %                          58.68
    L2 Cache Throughput                                                                  %                           1.23
    SM Active Cycles                                                                 cycle                   2,771,125.49
    Compute (SM) [%]                                                                     %                           1.78
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           4
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          4,096
    Waves Per SM                                                                                                     0.04
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              1
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          43.91
    Achieved Active Warps Per SM                                                      warp                          28.10
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,788
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 16, nThreads: 32
==PROF== Connected to process 1968052 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 16 blocks, 32 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1968052
[1968052] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:25:15, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.09
    gpu__time_duration.avg                                                         msecond                         221.39
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           3.41
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.98
    Elapsed Cycles                                                                   cycle                    169,359,316
    Memory [%]                                                                           %                           0.78
    DRAM Throughput                                                                      %                           0.09
    Duration                                                                       msecond                         221.39
    L1/TEX Cache Throughput                                                              %                           5.72
    L2 Cache Throughput                                                                  %                           0.63
    SM Active Cycles                                                                 cycle                  23,074,864.62
    Compute (SM) [%]                                                                     %                           1.20
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                            512
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             40
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory This kernel's  
          theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM The difference        
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,676
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 16, nThreads: 64
==PROF== Connected to process 1968795 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 16 blocks, 64 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1968795
[1968795] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:25:32, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.16
    gpu__time_duration.avg                                                         msecond                         121.73
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           6.26
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.79
    Elapsed Cycles                                                                   cycle                     93,097,861
    Memory [%]                                                                           %                           1.42
    DRAM Throughput                                                                      %                           0.16
    Duration                                                                       msecond                         121.73
    L1/TEX Cache Throughput                                                              %                          11.25
    L2 Cache Throughput                                                                  %                           1.14
    SM Active Cycles                                                                 cycle                  11,729,122.89
    Compute (SM) [%]                                                                     %                           2.18
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          1,024
    Waves Per SM                                                                                                     0.01
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             20
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           3.11
    Achieved Active Warps Per SM                                                      warp                           1.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (3.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,692
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 16, nThreads: 128
==PROF== Connected to process 1969168 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 16 blocks, 128 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1969168
[1969168] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:25:44, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.30
    gpu__time_duration.avg                                                         msecond                          63.89
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          12.21
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.73
    Elapsed Cycles                                                                   cycle                     48,859,249
    Memory [%]                                                                           %                           2.70
    DRAM Throughput                                                                      %                           0.30
    Duration                                                                       msecond                          63.89
    L1/TEX Cache Throughput                                                              %                          21.67
    L2 Cache Throughput                                                                  %                           2.15
    SM Active Cycles                                                                 cycle                   6,079,232.98
    Compute (SM) [%]                                                                     %                           4.15
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          2,048
    Waves Per SM                                                                                                     0.01
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             10
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           6.13
    Achieved Active Warps Per SM                                                      warp                           3.92
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (6.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,724
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 16, nThreads: 256
==PROF== Connected to process 1969620 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 16 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1969620
[1969620] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:25:54, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.37
    gpu__time_duration.avg                                                         msecond                          51.35
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           9.22
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.03
    Elapsed Cycles                                                                   cycle                     39,230,331
    Memory [%]                                                                           %                           3.39
    DRAM Throughput                                                                      %                           0.37
    Duration                                                                       msecond                          51.35
    L1/TEX Cache Throughput                                                              %                          35.66
    L2 Cache Throughput                                                                  %                           2.68
    SM Active Cycles                                                                 cycle                   3,724,247.06
    Compute (SM) [%]                                                                     %                           5.17
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          4,096
    Waves Per SM                                                                                                     0.03
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              5
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                          12.03
    Achieved Active Warps Per SM                                                      warp                           7.70
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (12.0%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,788
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 16, nThreads: 512
==PROF== Connected to process 1970026 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 16 blocks, 512 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1970026
[1970026] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:26:03, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.50
    gpu__time_duration.avg                                                         msecond                          38.42
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           9.25
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.82
    Elapsed Cycles                                                                   cycle                     29,382,335
    Memory [%]                                                                           %                           5.01
    DRAM Throughput                                                                      %                           0.50
    Duration                                                                       msecond                          38.42
    L1/TEX Cache Throughput                                                              %                          52.06
    L2 Cache Throughput                                                                  %                           3.69
    SM Active Cycles                                                                 cycle                   2,825,484.08
    Compute (SM) [%]                                                                     %                           6.90
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          8,192
    Waves Per SM                                                                                                     0.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          23.51
    Achieved Active Warps Per SM                                                      warp                          15.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers The difference     
          between calculated theoretical (50.0%) and measured achieved occupancy (23.5%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,915
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 16, nThreads: 1024
==PROF== Connected to process 1970200 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 16 blocks, 1024 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1970200
[1970200] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:26:13, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.25
    gpu__time_duration.avg                                                         msecond                          75.02
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           4.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         763.57
    Elapsed Cycles                                                                   cycle                     57,280,016
    Memory [%]                                                                           %                           2.84
    DRAM Throughput                                                                      %                           0.25
    Duration                                                                       msecond                          75.02
    L1/TEX Cache Throughput                                                              %                          58.53
    L2 Cache Throughput                                                                  %                           2.43
    SM Active Cycles                                                                 cycle                   2,779,154.06
    Compute (SM) [%]                                                                     %                           3.54
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          16
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         16,384
    Waves Per SM                                                                                                     0.15
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              1
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          43.84
    Achieved Active Warps Per SM                                                      warp                          28.06
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,765,171
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 64, nThreads: 32
==PROF== Connected to process 1970618 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 64 blocks, 32 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1970618
[1970618] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:26:25, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.31
    gpu__time_duration.avg                                                         msecond                          62.04
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          12.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.93
    Elapsed Cycles                                                                   cycle                     47,455,424
    Memory [%]                                                                           %                           2.78
    DRAM Throughput                                                                      %                           0.31
    Duration                                                                       msecond                          62.04
    L1/TEX Cache Throughput                                                              %                           5.72
    L2 Cache Throughput                                                                  %                           2.22
    SM Active Cycles                                                                 cycle                  23,079,510.31
    Compute (SM) [%]                                                                     %                           4.28
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          2,048
    Waves Per SM                                                                                                     0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             40
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,724
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 64, nThreads: 64
==PROF== Connected to process 1970940 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 64 blocks, 64 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1970940
[1970940] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:26:35, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.46
    gpu__time_duration.avg                                                         msecond                          41.67
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          18.29
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.06
    Elapsed Cycles                                                                   cycle                     31,881,670
    Memory [%]                                                                           %                           4.14
    DRAM Throughput                                                                      %                           0.46
    Duration                                                                       msecond                          41.67
    L1/TEX Cache Throughput                                                              %                          11.24
    L2 Cache Throughput                                                                  %                           3.29
    SM Active Cycles                                                                 cycle                  11,740,266.36
    Compute (SM) [%]                                                                     %                           6.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          4,096
    Waves Per SM                                                                                                     0.03
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             20
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           3.10
    Achieved Active Warps Per SM                                                      warp                           1.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (3.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,788
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 64, nThreads: 128
==PROF== Connected to process 1971687 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 64 blocks, 128 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1971687
[1971687] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:26:43, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.89
    gpu__time_duration.avg                                                         msecond                          21.50
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          36.31
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.63
    Elapsed Cycles                                                                   cycle                     16,437,676
    Memory [%]                                                                           %                           8.02
    DRAM Throughput                                                                      %                           0.89
    Duration                                                                       msecond                          21.50
    L1/TEX Cache Throughput                                                              %                          21.64
    L2 Cache Throughput                                                                  %                           6.37
    SM Active Cycles                                                                 cycle                      6,089,340
    Compute (SM) [%]                                                                     %                          12.34
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          8,192
    Waves Per SM                                                                                                     0.06
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             10
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           6.13
    Achieved Active Warps Per SM                                                      warp                           3.92
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (6.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,915
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 64, nThreads: 256
==PROF== Connected to process 1971989 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 64 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1971989
[1971989] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:26:51, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.75
    gpu__time_duration.avg                                                         msecond                          25.80
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          18.34
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.08
    Elapsed Cycles                                                                   cycle                     19,739,995
    Memory [%]                                                                           %                           6.73
    DRAM Throughput                                                                      %                           0.75
    Duration                                                                       msecond                          25.80
    L1/TEX Cache Throughput                                                              %                          35.62
    L2 Cache Throughput                                                                  %                           5.32
    SM Active Cycles                                                                 cycle                   3,726,727.44
    Compute (SM) [%]                                                                     %                          10.28
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         16,384
    Waves Per SM                                                                                                     0.12
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              5
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                          12.02
    Achieved Active Warps Per SM                                                      warp                           7.69
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (12.0%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,765,171
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 64, nThreads: 512
==PROF== Connected to process 1972082 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 64 blocks, 512 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1972082
[1972082] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:27:00, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.50
    gpu__time_duration.avg                                                         msecond                          38.41
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           9.26
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.40
    Elapsed Cycles                                                                   cycle                     29,361,217
    Memory [%]                                                                           %                           5.02
    DRAM Throughput                                                                      %                           0.50
    Duration                                                                       msecond                          38.41
    L1/TEX Cache Throughput                                                              %                          52.09
    L2 Cache Throughput                                                                  %                           3.69
    SM Active Cycles                                                                 cycle                   2,827,762.31
    Compute (SM) [%]                                                                     %                           6.91
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         32,768
    Waves Per SM                                                                                                     0.30
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          23.51
    Achieved Active Warps Per SM                                                      warp                          15.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers The difference     
          between calculated theoretical (50.0%) and measured achieved occupancy (23.5%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,765,683
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 64, nThreads: 1024
==PROF== Connected to process 1972393 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 64 blocks, 1024 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1972393
[1972393] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:27:11, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.26
    gpu__time_duration.avg                                                         msecond                          74.94
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           4.66
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.50
    Elapsed Cycles                                                                   cycle                     57,364,759
    Memory [%]                                                                           %                           2.84
    DRAM Throughput                                                                      %                           0.26
    Duration                                                                       msecond                          74.94
    L1/TEX Cache Throughput                                                              %                          58.60
    L2 Cache Throughput                                                                  %                           2.43
    SM Active Cycles                                                                 cycle                   2,776,876.76
    Compute (SM) [%]                                                                     %                           3.54
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                          64
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         65,536
    Waves Per SM                                                                                                     0.59
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              1
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          43.85
    Achieved Active Warps Per SM                                                      warp                          28.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,766,707
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 256, nThreads: 32
==PROF== Connected to process 1973403 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 256 blocks, 32 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1973403
[1973403] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:27:19, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.92
    gpu__time_duration.avg                                                         msecond                          21.06
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          36.13
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.16
    Elapsed Cycles                                                                   cycle                     16,093,611
    Memory [%]                                                                           %                           8.20
    DRAM Throughput                                                                      %                           0.92
    Duration                                                                       msecond                          21.06
    L1/TEX Cache Throughput                                                              %                           8.45
    L2 Cache Throughput                                                                  %                           6.52
    SM Active Cycles                                                                 cycle                  15,623,434.60
    Compute (SM) [%]                                                                     %                          12.61
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                          8,192
    Waves Per SM                                                                                                     0.07
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             40
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.33
    Achieved Active Warps Per SM                                                      warp                           1.49
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,764,915
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 256, nThreads: 64
==PROF== Connected to process 1974010 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 256 blocks, 64 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1974010
[1974010] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:27:27, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.78
    gpu__time_duration.avg                                                         msecond                          25.10
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          29.43
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.20
    SM Frequency                                                             cycle/usecond                         756.71
    Elapsed Cycles                                                                   cycle                     18,991,886
    Memory [%]                                                                           %                           6.94
    DRAM Throughput                                                                      %                           0.78
    Duration                                                                       msecond                          25.10
    L1/TEX Cache Throughput                                                              %                          14.24
    L2 Cache Throughput                                                                  %                           5.51
    SM Active Cycles                                                                 cycle                   9,257,696.63
    Compute (SM) [%]                                                                     %                          10.68
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         16,384
    Waves Per SM                                                                                                     0.12
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             20
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           4.03
    Achieved Active Warps Per SM                                                      warp                           2.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (4.0%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,765,171
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 256, nThreads: 128
==PROF== Connected to process 1974470 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 256 blocks, 128 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1974470
[1974470] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:27:35, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.73
    gpu__time_duration.avg                                                         msecond                          26.31
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          18.91
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.56
    Elapsed Cycles                                                                   cycle                     20,114,902
    Memory [%]                                                                           %                           6.61
    DRAM Throughput                                                                      %                           0.73
    Duration                                                                       msecond                          26.31
    L1/TEX Cache Throughput                                                              %                          33.96
    L2 Cache Throughput                                                                  %                           5.21
    SM Active Cycles                                                                 cycle                   3,915,068.10
    Compute (SM) [%]                                                                     %                          10.09
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         32,768
    Waves Per SM                                                                                                     0.24
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             10
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                          11.28
    Achieved Active Warps Per SM                                                      warp                           7.22
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (11.3%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,765,683
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 256, nThreads: 256
==PROF== Connected to process 1974599 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 2 passes

==ERROR== LaunchFailed

==ERROR== LaunchFailed
==PROF== Trying to shutdown target application
==ERROR== The application returned an error code (9).
==ERROR== An error occurred while trying to profile.
[1974599] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:27:43, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                                            (!) n/a
    gpu__time_duration.avg                                                                                        (!) n/a
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                                         (!) n/a
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                                                                (!) n/a
    SM Frequency                                                                                                  (!) n/a
    Elapsed Cycles                                                                                                (!) n/a
    Memory [%]                                                                                                    (!) n/a
    DRAM Throughput                                                                                               (!) n/a
    Duration                                                                                                      (!) n/a
    L1/TEX Cache Throughput                                                                                       (!) n/a
    L2 Cache Throughput                                                                                           (!) n/a
    SM Active Cycles                                                                                              (!) n/a
    Compute (SM) [%]                                                                                              (!) n/a
    ---------------------------------------------------------------------- --------------- ------------------------------
    ERR   Rule Bottleneck returned an error: Metric sm__throughput.avg.pct_of_peak_sustained_elapsed not found          
    ----- --------------------------------------------------------------------------------------------------------------
    ERR   <built-in function IAction_metric_by_name> returned a result with an error set                                
          /global/homes/s/shubhp/Documents/NVIDIA Nsight Compute/2022.1.1/Sections/SpeedOfLight.py:79                   
          /global/common/software/nersc/pm-2021q4/easybuild/software/Nsight-Compute/2022.1.1/target/linux-desktop-glibc_
          2_11_3-x64/../../sections/NvRules.py:1969                                                                     

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         65,536
    Waves Per SM                                                                                                     0.47
    ---------------------------------------------------------------------- --------------- ------------------------------
    ERR   Rule Launch Configuration returned an error: Metric sm__warps_active.avg.pct_of_peak_sustained_active not     
          found                                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    ERR   <built-in function IAction_metric_by_name> returned a result with an error set                                
          /global/homes/s/shubhp/Documents/NVIDIA Nsight Compute/2022.1.1/Sections/LaunchStatistics.py:50               
          /global/common/software/nersc/pm-2021q4/easybuild/software/Nsight-Compute/2022.1.1/target/linux-desktop-glibc_
          2_11_3-x64/../../sections/NvRules.py:1969                                                                     

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              5
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                                            (!) n/a
    Achieved Active Warps Per SM                                                                                  (!) n/a
    ---------------------------------------------------------------------- --------------- ------------------------------
    ERR   Rule Occupancy returned an error: Metric sm__warps_active.avg.pct_of_peak_sustained_active not found          
    ----- --------------------------------------------------------------------------------------------------------------
    ERR   <built-in function IAction_metric_by_name> returned a result with an error set                                
          /global/homes/s/shubhp/Documents/NVIDIA Nsight Compute/2022.1.1/Sections/Occupancy.py:45                      
          /global/common/software/nersc/pm-2021q4/easybuild/software/Nsight-Compute/2022.1.1/target/linux-desktop-glibc_
          2_11_3-x64/../../sections/NvRules.py:1969                                                                     

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                        no data
    Branch Instructions                                                                                           (!) n/a
    Branch Efficiency                                                                                             (!) n/a
    Avg. Divergent Branches                                                                                       (!) n/a
    ---------------------------------------------------------------------- --------------- ------------------------------
    ERR   Rule PC sampling data returned an error: Metric smsp__pcsamp_sample_count not found                           
    ----- --------------------------------------------------------------------------------------------------------------
    ERR   <built-in function IAction_metric_by_name> returned a result with an error set                                
          /global/homes/s/shubhp/Documents/NVIDIA Nsight Compute/2022.1.1/Sections/PCSamplingData.py:47                 
          /global/common/software/nersc/pm-2021q4/easybuild/software/Nsight-Compute/2022.1.1/target/linux-desktop-glibc_
          2_11_3-x64/../../sections/NvRules.py:1969                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 256, nThreads: 512
==PROF== Connected to process 1974873 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 256 blocks, 512 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1974873
[1974873] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:27:54, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.26
    gpu__time_duration.avg                                                         msecond                          72.32
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           4.66
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.18
    Elapsed Cycles                                                                   cycle                     55,264,397
    Memory [%]                                                                           %                           2.78
    DRAM Throughput                                                                      %                           0.26
    Duration                                                                       msecond                          72.32
    L1/TEX Cache Throughput                                                              %                          57.32
    L2 Cache Throughput                                                                  %                           2.18
    SM Active Cycles                                                                 cycle                   2,682,742.24
    Compute (SM) [%]                                                                     %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        131,072
    Waves Per SM                                                                                                     1.19
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 39 thread blocks.   
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          37.44
    Achieved Active Warps Per SM                                                      warp                          23.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers The difference     
          between calculated theoretical (50.0%) and measured achieved occupancy (37.4%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,768,755
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 256, nThreads: 1024
==PROF== Connected to process 1975238 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 256 blocks, 1024 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1975238
[1975238] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:28:04, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.25
    gpu__time_duration.avg                                                         msecond                          74.89
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           4.67
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.72
    Elapsed Cycles                                                                   cycle                     57,348,120
    Memory [%]                                                                           %                           2.84
    DRAM Throughput                                                                      %                           0.25
    Duration                                                                       msecond                          74.89
    L1/TEX Cache Throughput                                                              %                          58.54
    L2 Cache Throughput                                                                  %                           2.43
    SM Active Cycles                                                                 cycle                   2,780,335.40
    Compute (SM) [%]                                                                     %                           3.54
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         256
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        262,144
    Waves Per SM                                                                                                     2.37
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              1
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          43.86
    Achieved Active Warps Per SM                                                      warp                          28.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,772,851
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1024, nThreads: 32
==PROF== Connected to process 1975443 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1024 blocks, 32 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1975443
[1975443] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:28:12, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.90
    gpu__time_duration.avg                                                         msecond                          21.06
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          36.26
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.07
    Elapsed Cycles                                                                   cycle                     16,092,513
    Memory [%]                                                                           %                           8.20
    DRAM Throughput                                                                      %                           0.90
    Duration                                                                       msecond                          21.06
    L1/TEX Cache Throughput                                                              %                          11.23
    L2 Cache Throughput                                                                  %                           6.51
    SM Active Cycles                                                                 cycle                  11,749,731.52
    Compute (SM) [%]                                                                     %                          12.61
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         32,768
    Waves Per SM                                                                                                     0.30
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             40
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           3.10
    Achieved Active Warps Per SM                                                      warp                           1.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory This kernel's  
          theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM The difference        
          between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,765,683
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1024, nThreads: 64
==PROF== Connected to process 1975860 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1024 blocks, 64 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1975860
[1975860] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:28:21, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.77
    gpu__time_duration.avg                                                         msecond                          25.06
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          27.45
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.61
    Elapsed Cycles                                                                   cycle                     19,185,043
    Memory [%]                                                                           %                           6.87
    DRAM Throughput                                                                      %                           0.77
    Duration                                                                       msecond                          25.06
    L1/TEX Cache Throughput                                                              %                          19.51
    L2 Cache Throughput                                                                  %                           5.46
    SM Active Cycles                                                                 cycle                   6,756,087.47
    Compute (SM) [%]                                                                     %                          10.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         65,536
    Waves Per SM                                                                                                     0.47
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             20
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           5.77
    Achieved Active Warps Per SM                                                      warp                           3.69
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (5.8%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,766,707
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1024, nThreads: 128
==PROF== Connected to process 1975959 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1024 blocks, 128 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1975959
[1975959] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:28:29, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.73
    gpu__time_duration.avg                                                         msecond                          26.34
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          18.33
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         763.67
    Elapsed Cycles                                                                   cycle                     20,114,758
    Memory [%]                                                                           %                           6.61
    DRAM Throughput                                                                      %                           0.73
    Duration                                                                       msecond                          26.34
    L1/TEX Cache Throughput                                                              %                          35.02
    L2 Cache Throughput                                                                  %                           5.22
    SM Active Cycles                                                                 cycle                   3,798,393.13
    Compute (SM) [%]                                                                     %                          10.09
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        131,072
    Waves Per SM                                                                                                     0.95
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             10
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                          11.72
    Achieved Active Warps Per SM                                                      warp                           7.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (11.7%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,768,755
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1024, nThreads: 256
==PROF== Connected to process 1976273 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1024 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1976273
[1976273] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:28:38, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.47
    gpu__time_duration.avg                                                         msecond                          40.72
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           9.28
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.07
    Elapsed Cycles                                                                   cycle                     31,110,351
    Memory [%]                                                                           %                           4.61
    DRAM Throughput                                                                      %                           0.47
    Duration                                                                       msecond                          40.72
    L1/TEX Cache Throughput                                                              %                          47.86
    L2 Cache Throughput                                                                  %                           3.43
    SM Active Cycles                                                                 cycle                   2,996,752.01
    Compute (SM) [%]                                                                     %                           6.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        262,144
    Waves Per SM                                                                                                     1.90
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 483 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 66.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              5
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                          21.03
    Achieved Active Warps Per SM                                                      warp                          13.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (21.0%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,772,851
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1024, nThreads: 512
==PROF== Connected to process 1976617 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1024 blocks, 512 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1976617
[1976617] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:28:49, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.26
    gpu__time_duration.avg                                                         msecond                          72.32
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           4.64
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         766.25
    Elapsed Cycles                                                                   cycle                     55,414,682
    Memory [%]                                                                           %                           2.77
    DRAM Throughput                                                                      %                           0.26
    Duration                                                                       msecond                          72.32
    L1/TEX Cache Throughput                                                              %                          57.19
    L2 Cache Throughput                                                                  %                           2.17
    SM Active Cycles                                                                 cycle                   2,681,919.21
    Compute (SM) [%]                                                                     %                           3.66
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        524,288
    Waves Per SM                                                                                                     4.74
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 4 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 20.0% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          37.44
    Achieved Active Warps Per SM                                                      warp                          23.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers The difference     
          between calculated theoretical (50.0%) and measured achieved occupancy (37.4%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,781,043
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 1024, nThreads: 1024
==PROF== Connected to process 1976708 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 1024 blocks, 1024 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1976708
[1976708] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:28:59, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.26
    gpu__time_duration.avg                                                         msecond                          74.94
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           4.66
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         764.92
    Elapsed Cycles                                                                   cycle                     57,319,995
    Memory [%]                                                                           %                           2.84
    DRAM Throughput                                                                      %                           0.26
    Duration                                                                       msecond                          74.94
    L1/TEX Cache Throughput                                                              %                          58.47
    L2 Cache Throughput                                                                  %                           2.43
    SM Active Cycles                                                                 cycle                   2,783,476.97
    Compute (SM) [%]                                                                     %                           3.54
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       1,024
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      1,048,576
    Waves Per SM                                                                                                     9.48
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              1
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          43.81
    Achieved Active Warps Per SM                                                      warp                          28.04
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,797,427
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4096, nThreads: 32
==PROF== Connected to process 1977057 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4096 blocks, 32 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1977057
[1977057] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:29:07, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.91
    gpu__time_duration.avg                                                         msecond                          21.08
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          36.24
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.21
    SM Frequency                                                             cycle/usecond                         763.82
    Elapsed Cycles                                                                   cycle                     16,100,029
    Memory [%]                                                                           %                           8.20
    DRAM Throughput                                                                      %                           0.91
    Duration                                                                       msecond                          21.08
    L1/TEX Cache Throughput                                                              %                          11.23
    L2 Cache Throughput                                                                  %                           6.50
    SM Active Cycles                                                                 cycle                  11,748,410.71
    Compute (SM) [%]                                                                     %                          12.60
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        131,072
    Waves Per SM                                                                                                     1.19
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 639 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 93.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             40
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           3.10
    Achieved Active Warps Per SM                                                      warp                           1.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,768,755
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4096, nThreads: 64
==PROF== Connected to process 1977715 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4096 blocks, 64 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1977715
[1977715] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:29:16, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.77
    gpu__time_duration.avg                                                         msecond                          24.98
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          27.95
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         767.89
    Elapsed Cycles                                                                   cycle                     19,181,250
    Memory [%]                                                                           %                           6.87
    DRAM Throughput                                                                      %                           0.77
    Duration                                                                       msecond                          24.98
    L1/TEX Cache Throughput                                                              %                          19.86
    L2 Cache Throughput                                                                  %                           5.47
    SM Active Cycles                                                                 cycle                   6,635,603.26
    Compute (SM) [%]                                                                     %                          10.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         64
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        262,144
    Waves Per SM                                                                                                     1.90
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 1935 thread         
          blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may       
          account for up to 50.0% of the total kernel runtime with a lower occupancy of 90.7%. Try launching a grid     
          with no partial wave. The overall impact of this tail effect also lessens with the number of full waves       
          executed for a grid. See the Hardware Model                                                                   
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             20
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             32
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                           5.83
    Achieved Active Warps Per SM                                                      warp                           3.73
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (5.8%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,772,851
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4096, nThreads: 128
==PROF== Connected to process 1978131 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4096 blocks, 128 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1978131
[1978131] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:29:24, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.73
    gpu__time_duration.avg                                                         msecond                          26.46
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                          18.26
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.19
    SM Frequency                                                             cycle/usecond                         751.62
    Elapsed Cycles                                                                   cycle                     19,891,180
    Memory [%]                                                                           %                           6.68
    DRAM Throughput                                                                      %                           0.73
    Duration                                                                       msecond                          26.46
    L1/TEX Cache Throughput                                                              %                          35.48
    L2 Cache Throughput                                                                  %                           5.27
    SM Active Cycles                                                                 cycle                   3,745,812.46
    Compute (SM) [%]                                                                     %                          10.20
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        524,288
    Waves Per SM                                                                                                     3.79
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 855 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 80.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             10
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                          11.98
    Achieved Active Warps Per SM                                                      warp                           7.67
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (12.0%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,781,043
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4096, nThreads: 256
==PROF== Connected to process 1978433 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4096 blocks, 256 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1978433
[1978433] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:29:33, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.47
    gpu__time_duration.avg                                                         msecond                          40.71
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           9.26
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         766.17
    Elapsed Cycles                                                                   cycle                     31,189,097
    Memory [%]                                                                           %                           4.60
    DRAM Throughput                                                                      %                           0.47
    Duration                                                                       msecond                          40.71
    L1/TEX Cache Throughput                                                              %                          47.88
    L2 Cache Throughput                                                                  %                           3.42
    SM Active Cycles                                                                 cycle                   2,995,476.61
    Compute (SM) [%]                                                                     %                           6.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      1,048,576
    Waves Per SM                                                                                                     7.59
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              5
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             40
    Theoretical Occupancy                                                                %                          62.50
    Achieved Occupancy                                                                   %                          21.03
    Achieved Active Warps Per SM                                                      warp                          13.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers The difference     
          between calculated theoretical (62.5%) and measured achieved occupancy (21.0%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,797,427
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4096, nThreads: 512
==PROF== Connected to process 1978524 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4096 blocks, 512 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1978524
[1978524] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:29:43, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.27
    gpu__time_duration.avg                                                         msecond                          72.25
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           4.66
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         765.02
    Elapsed Cycles                                                                   cycle                     55,276,518
    Memory [%]                                                                           %                           2.78
    DRAM Throughput                                                                      %                           0.27
    Duration                                                                       msecond                          72.25
    L1/TEX Cache Throughput                                                              %                          57.27
    L2 Cache Throughput                                                                  %                           2.17
    SM Active Cycles                                                                 cycle                   2,683,624.12
    Compute (SM) [%]                                                                     %                           3.67
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                          16.38
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      2,097,152
    Waves Per SM                                                                                                    18.96
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              2
    Block Limit Shared Mem                                                           block                             16
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          37.42
    Achieved Active Warps Per SM                                                      warp                          23.95
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers The difference     
          between calculated theoretical (50.0%) and measured achieved occupancy (37.4%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,830,195
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

nblocks: 4096, nThreads: 1024
==PROF== Connected to process 1978863 (/pscratch/sd/s/shubhp/sobel-cuda-omp/build/sobel_gpu)
==PROF== Profiling "_Z16sobel_kernel_gpuPfS_iiiS_S_" - 1: 0%....50%....100% - 13 passes
 Read data from the file ../data/zebra-gray-int8-4x 
 GPU configuration: 4096 blocks, 1024 threads per block 
 Wrote the output file ../data/processed-raw-int8-4x-gpu.dat 
==PROF== Disconnected from process 1978863
[1978863] sobel_gpu@127.0.0.1
  _Z16sobel_kernel_gpuPfS_iiiS_S_, 2023-Nov-05 22:29:53, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    dram__throughput.avg.pct_of_peak_sustained_elapsed                                   %                           0.25
    gpu__time_duration.avg                                                         msecond                          74.98
    smsp__cycles_active.avg.pct_of_peak_sustained_elapsed                                %                           4.66
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.22
    SM Frequency                                                             cycle/usecond                         766.61
    Elapsed Cycles                                                                   cycle                     57,484,236
    Memory [%]                                                                           %                           2.83
    DRAM Throughput                                                                      %                           0.25
    Duration                                                                       msecond                          74.98
    L1/TEX Cache Throughput                                                              %                          58.45
    L2 Cache Throughput                                                                  %                           2.42
    SM Active Cycles                                                                 cycle                   2,785,294.96
    Compute (SM) [%]                                                                     %                           3.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                      1,024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                       4,096
    Registers Per Thread                                                   register/thread                             44
    Shared Memory Configuration Size                                                 Kbyte                           8.19
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                      4,194,304
    Waves Per SM                                                                                                    37.93
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              1
    Block Limit Shared Mem                                                           block                              8
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                          43.81
    Achieved Active Warps Per SM                                                      warp                          28.04
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

    Section: Source Counters
    ---------------------------------------------------------------------- --------------- ------------------------------
    Branch Instructions Ratio                                                            %                           0.04
    Branch Instructions                                                               inst                     21,895,731
    Branch Efficiency                                                                    %                          87.36
    Avg. Divergent Branches                                                                                      4,096.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel has uncoalesced global accesses resulting in a total of 416120780 excessive sectors (85% of the   
          total 489394778 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   

